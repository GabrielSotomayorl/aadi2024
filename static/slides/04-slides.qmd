---
title: "Clase 4 <br> Regresi√≥n Lineal Simple II"
subtitle: "An√°lisis Avanzado de Datos"
author: "Gabriel Sotomayor"
format: 
  revealjs:
    theme: [dark, custom.scss]
    slide-number: true
    auto-fit: true
    logo: images/logo.png
editor: visual
---

# Recordatorio de la clase anterior

## Recta de Regresi√≥n M√≠nimo-Cuadr√°tica {.smaller background-color="white"}

La regresi√≥n lineal simple se utiliza para describir la relaci√≥n entre dos variables, una independiente (explicativa) y una dependiente (respuesta), mediante una recta de regresi√≥n. A diferencia de la correlaci√≥n si asume una direccionalidad.

#### F√≥rmula General {background-color="white"}

La recta de regresi√≥n se expresa como: 
$$
\hat{y} = a + bx
$$


**Pendiente ùëè:** Indica el cambio promedio en la variable respuestaùë¶por cada unidad de cambio en la variable explicativa ùë•.\
**Ordenada en el origen ùëé:** Representa el valor predicho deùë¶cuando ùë•= 0. S√≥lo tiene significado estad√≠stico cuando x toma valores cercanos a 0.

## Caracter√≠sticas de la Regresi√≥n {.smaller background-color="white"}

-   **Distinci√≥n entre variable explicativa y variable respuesta**:
    -   La regresi√≥n m√≠nimo-cuadr√°tica considera s√≥lo las distancias verticales de los puntos a la recta.
    -   Cambiar los papeles de las dos variables resulta en una recta de regresi√≥n distinta.

## Caracter√≠sticas de la Regresi√≥n {.smaller background-color="white"}

**Conexi√≥n entre correlaci√≥n y regresi√≥n**: - La pendiente de la recta de regresi√≥n m√≠nimo-cuadr√°tica se calcula como:

$$
  b = r \frac{s_y}{s_x}
$$

-   A lo largo de la recta de regresi√≥n:
    -   Un cambio de una desviaci√≥n t√≠pica en x provoca un cambio de r desviaciones t√≠picas en y.
    -   Cuando r = 1 o r = ‚àí1, el cambio en y predicho es igual al cambio en x, en t√©rminos de desviaciones est√°ndar.
    -   Si ‚àí1 ‚â§ r ‚â§ 1, el cambio en y es menor que el cambio en x.
    -   A menor correlaci√≥n, menor es la predicci√≥n de y en respuesta a x.

## Caracter√≠sticas de la Regresi√≥n {.smaller background-color="white"}

-   **Punto de paso de la recta de regresi√≥n**:
    -   La recta de regresi√≥n m√≠nimo-cuadr√°tica siempre pasa por el punto $(\bar{x}, \bar{y})$.

    -   La recta de regresi√≥n se describe completamente con $\bar{x}$, $s_x$, $\bar{y}$, $s_y$ y $r$.

    -   **Correlaci√≥n r y la fuerza de la relaci√≥n lineal**:

    -   El cuadrado de la correlaci√≥n, $r^2$, indica la fracci√≥n de la variaci√≥n de $y$ explicada por la recta de regresi√≥n.

    -   $r^2$ se utiliza para medir la calidad de la predicci√≥n proporcionada por la regresi√≥n.
-   **Relaci√≥n entre r y** $r^2$:
    -   Una correlaci√≥n perfecta ($r = \pm1$) implica que $r^2 = 1$, lo que significa que toda la variaci√≥n de $y$ se explica por la relaci√≥n lineal con $x$.
    -   Si $r = \pm0.7$, entonces $r^2 = 0.49$, indicando que aproximadamente la mitad de la variaci√≥n se explica con la relaci√≥n lineal.

## [Evaluaciones]{style="color:#FFFFFF;"} {.dark-background}

**Tarea 1: 4 de septiembre (la pauta est√° arriba)**\
- Gesti√≥n de datos\
- Estad√≠stica bivariada\
- Regresi√≥n lineal simple

**Prueba 1: 9 de Septiembre**\
- Uso de modelos en ciencias sociales\
- Estad√≠stica bivariada\
- Regresi√≥n lineal simple

## Objetivo de la sesi√≥n {.center background-color="white"}

Profundizar en la interpretaci√≥n de la regresi√≥n lineal simple y el an√°lisis de los residuos.

## Residuos: Definici√≥n y Significado {.smaller background-color="white"}

*¬øQu√© son los Residuos?*

Un residuo es la diferencia entre el valor observado de la variable respuesta (Y) y el valor predicho por la recta de regresi√≥n ($\hat{y}$).\
F√≥rmula: residuo = Y observada ‚àí Y predicha = $Y$ ‚àí $\hat{y}$.

*Importancia de los Residuos:*

-   Los residuos representan las desviaciones de los datos respecto a la recta de regresi√≥n, indicando qu√© tan bien el modelo captura la relaci√≥n entre X e Y.\
-   La media de los residuos es siempre cero, lo que significa que, en promedio, los puntos est√°n equidistantes de la recta.

## Gr√°fico de dispersi√≥n con recta de regresi√≥n {.center background-color="white"}

```{r}
library(ggplot2)
library(dplyr)

# Crear categor√≠as de escolaridad para calcular medias condicionales
datos <- readRDS("data/datos.rds") %>% 
  filter(brecha<40 & brecha > -20 & !is.na(brecha))
datos <- datos %>%
  mutate(escolaridad_cat = cut(promedio_anios_escolaridad25_2017, 
                               breaks = seq(floor(min(promedio_anios_escolaridad25_2017, na.rm = TRUE)), 
                                            ceiling(max(promedio_anios_escolaridad25_2017, na.rm = TRUE)), 
                                            by = 1), 
                               include.lowest = TRUE))

mean_brecha <- mean(datos$brecha, na.rm = TRUE)

ggplot(datos, aes(x = promedio_anios_escolaridad25_2017, y = brecha)) +
  geom_point(color = "#0073C2", size = 3, alpha = 0.7) +  # Puntos m√°s grandes y ligeramente transparentes
  geom_smooth(method = "lm", color = "black", linetype = "solid", se = FALSE) +  # L√≠nea de tendencia en negro
  geom_vline(xintercept = mean(datos$promedio_anios_escolaridad25_2017, na.rm = TRUE), color = "red", linetype = "dotted", size = 1) +  # L√≠nea roja punteada en la media de 'promedio_anios_escolaridad25_2017'
  geom_hline(yintercept = mean(datos$brecha, na.rm = TRUE), color = "red", linetype = "dotted", size = 1) +  # L√≠nea roja punteada en la media de 'brecha'
  labs(
    x = "Promedio de A√±os de Escolaridad (2017)",
    y = "Brecha Salarial de G√©nero (%)",
    title = "Relaci√≥n entre Promedio de A√±os de Escolaridad y Brecha Salarial de G√©nero"
  ) +
  theme_minimal(base_size = 16) +  # Tama√±o base de letra aumentado
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18),  # T√≠tulo centrado, en negrita y m√°s grande
    axis.title = element_text(face = "bold", size = 14),               # T√≠tulos de los ejes en negrita y m√°s grandes
    axis.text = element_text(color = "#333333", size = 12),            # Texto de los ejes m√°s grande
    panel.grid.major = element_line(color = "#e0e0e0"),                # L√≠neas de la cuadr√≠cula mayor en gris claro
    panel.grid.minor = element_blank()                                 # Elimina las l√≠neas de la cuadr√≠cula menor
  )
```

$$
\hat{brecha} = -16.451 + 2.852 * escolaridad
$$

## Componentes del $Y$ observado {.smaller background-color="white"}

Despu√©s de realizar una regresi√≥n, podemos descomponer $Y_i$ en tres componentes:

$$
Y_i = \bar{Y} + (\hat{Y}_i ‚àí \bar{Y}) + (Y_i ‚àí \hat{Y}_i)
$$

Donde:

- **Media General ($\bar{Y}$)**: Es constante para todas las observaciones.
- **Componente Explicado ($\hat{Y}_i ‚àí \bar{Y}$)**: Es la parte de $Y_i$ que es explicada por el modelo a partir de $X$.
- **Componente No Explicado (Residuos, $Y_i ‚àí \hat{Y}_i$)**: Es la parte de $Y_i$ que no es explicada por el modelo, representando el error de predicci√≥n.


## Componentes del Y observado {.smaller background-color="white"}

Por ejemplo: La comuna de Sierra Gorda tiene una brecha salarial de g√©nero de 31,13%, y una escolaridad de 13,19 a√±os. Para ese caso el modelo predice un valor de -16,451+2,852\*13,19 = 21,17. Es decir el residuo es igual a 31,13 - 21,17 = 9,96.\
$$
\text{Brecha de Sierra Gorda} = 11,42\% \, (\text{Media General}) \\
+ 9,75\% \, (\text{Componente Explicado}) \\
+ 9,96\% \, (\text{Residuos}) \\
= 31,13\%
$$


## Gr√°fico de componentes de Y {.center background-color="white"}

```{r}
# Crear categor√≠as de escolaridad para calcular medias condicionales
datos <- readRDS("data/datos.rds") %>% 
  filter(brecha < 40 & brecha > -20 & !is.na(brecha))
datos <- datos %>%
  mutate(escolaridad_cat = cut(promedio_anios_escolaridad25_2017, 
                               breaks = seq(floor(min(promedio_anios_escolaridad25_2017, na.rm = TRUE)), 
                                            ceiling(max(promedio_anios_escolaridad25_2017, na.rm = TRUE)), 
                                            by = 1), 
                               include.lowest = TRUE))

# Calcular la media de brecha salarial
mean_brecha <- mean(datos$brecha, na.rm = TRUE)

# Extraer el punto espec√≠fico cuya brecha es igual a 31.13078
punto_sierra_gorda <- datos %>% filter(abs(brecha - 31.13078) < 0.001)

# Calcular el valor predicho para la escolaridad de ese punto
prediccion_sierra_gorda <- -16.451 + 2.852 * punto_sierra_gorda$promedio_anios_escolaridad25_2017

# Crear el gr√°fico con las l√≠neas adicionales
ggplot(datos, aes(x = promedio_anios_escolaridad25_2017, y = brecha)) +
  geom_point(color = "#0073C2", size = 3, alpha = 0.7) +  # Puntos m√°s grandes y ligeramente transparentes
  geom_smooth(method = "lm", color = "black", linetype = "solid", se = FALSE) +  # L√≠nea de tendencia en negro
  geom_vline(xintercept = mean(datos$promedio_anios_escolaridad25_2017, na.rm = TRUE), color = "red", linetype = "dotted", size = 1) +  # L√≠nea roja punteada en la media de 'promedio_anios_escolaridad25_2017'
  geom_hline(yintercept = mean_brecha, color = "red", linetype = "dotted", size = 1) +  # L√≠nea roja punteada en la media de 'brecha'
  
  # Agregar l√≠neas para el punto espec√≠fico de Sierra Gorda
  geom_segment(aes(x = punto_sierra_gorda$promedio_anios_escolaridad25_2017, 
                   xend = punto_sierra_gorda$promedio_anios_escolaridad25_2017, 
                   y = mean_brecha, 
                   yend = prediccion_sierra_gorda), 
               color = "orange", linetype = "dashed", size = 1.2) +  # L√≠nea del componente explicado
  
  geom_segment(aes(x = punto_sierra_gorda$promedio_anios_escolaridad25_2017, 
                   xend = punto_sierra_gorda$promedio_anios_escolaridad25_2017, 
                   y = prediccion_sierra_gorda, 
                   yend = punto_sierra_gorda$brecha), 
               color = "purple", linetype = "dashed", size = 1.2) +  # L√≠nea del residuo

  geom_point(data = punto_sierra_gorda, aes(x = promedio_anios_escolaridad25_2017, y = brecha), 
             color = "darkred", size = 4) +  # Resaltar el punto espec√≠fico
  
  labs(
    x = "Promedio de A√±os de Escolaridad (2017)",
    y = "Brecha Salarial de G√©nero (%)",
    title = "Relaci√≥n entre Promedio de A√±os de Escolaridad y Brecha Salarial de G√©nero"
  ) +
  theme_minimal(base_size = 16) +  # Tama√±o base de letra aumentado
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18),  # T√≠tulo centrado, en negrita y m√°s grande
    axis.title = element_text(face = "bold", size = 14),               # T√≠tulos de los ejes en negrita y m√°s grandes
    axis.text = element_text(color = "#333333", size = 12),            # Texto de los ejes m√°s grande
    panel.grid.major = element_line(color = "#e0e0e0"),                # L√≠neas de la cuadr√≠cula mayor en gris claro
    panel.grid.minor = element_blank()                                 # Elimina las l√≠neas de la cuadr√≠cula menor
  )
```

## Residuos como Y ajustado por X {.smaller background-color="white"}

Cuando se predice Y a partir de X, los residuos pueden interpretarse como una nueva variable: Y ajustado por X.

Interpretaci√≥n de los Residuos:

-   Si un individuo tiene un residuo positivo, esto indica que su valor de Y es mayor de lo esperado dado su valor de X.\
-   Si un individuo tiene un residuo negativo, esto indica que su valor de Y es mayor de lo esperado dado su valor de X.

## Diagrama de Residuos {.smaller background-color="white"}

*¬øQu√© es un Diagrama de Residuos?*

Un diagrama de residuos es un gr√°fico de los residuos contra los valores predichos de la variable dependiente. Es una herramienta clave para evaluar el ajuste del modelo.

**Interpretaci√≥n del Diagrama de Residuos:**

-   Distribuci√≥n Uniforme: Indica un buen ajuste del modelo. Este caso se denomina homocedasticidad.
-   Formas Curvas: Se√±alan que la relaci√≥n no es lineal.
-   Dispersi√≥n Creciente/Decreciente: Indica variabilidad no constante en Y, lo que puede afectar la precisi√≥n de las predicciones. Esto se denomina heterocedasticidad.

## Digrama de residuos {.smaller background-color="white"}

```{r}
# Crear el modelo de regresi√≥n lineal
modelo <- lm(brecha ~ promedio_anios_escolaridad25_2017, data = datos)

# Calcular los residuos
datos$residuos <- resid(modelo)
datos$predicciones <- predict(modelo)

# Crear el gr√°fico de residuos
ggplot(datos, aes(x = predicciones, y = residuos)) +
  geom_point(color = "#0073C2", size = 3, alpha = 0.7) +  # Puntos m√°s grandes y ligeramente transparentes
  geom_hline(yintercept = 0, color = "red", linetype = "dotted", size = 1) +  # L√≠nea en y=0 para referencia
  labs(
    x = "Valores Predichos",
    y = "Residuos",
    title = "Gr√°fico de Residuos",
    subtitle = "Brecha Salarial de G√©nero vs. A√±os de Escolaridad"
  ) +
  theme_minimal(base_size = 16) +  # Tama√±o base de letra aumentado
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 18),  # T√≠tulo centrado, en negrita y m√°s grande
    plot.subtitle = element_text(hjust = 0.5, face = "italic", size = 14),  # Subt√≠tulo centrado y en cursiva
    axis.title = element_text(face = "bold", size = 14),  # T√≠tulos de los ejes en negrita y m√°s grandes
    axis.text = element_text(color = "#333333", size = 12),  # Texto de los ejes m√°s grande
    panel.grid.major = element_line(color = "#e0e0e0"),  # L√≠neas de la cuadr√≠cula mayor en gris claro
    panel.grid.minor = element_blank()  # Elimina las l√≠neas de la cuadr√≠cula menor
  )

```

## Diagramas de residuos {.smaller background-color="white"}

![](img/04/residuos.jpg) {.smaller background-color="white"}

## Coeficiente de Determinaci√≥n $R^2$ y Varianza Residual {.smaller background-color="white"}

**¬øQu√© es** $R^2$? - $R^2$, conocido como el coeficiente de determinaci√≥n, es una medida estad√≠stica que indica la proporci√≥n de la varianza en la variable dependiente $Y$ que es explicada por la variable independiente $X$ en un modelo de regresi√≥n.

-   Se calcula como: $$
    R^2 = 1 - \frac{\text{Varianza Residual}}{\text{Varianza Total de } Y}
    $$ Donde:
    -   **Varianza Residual:** Es la varianza de los residuos, es decir, la parte de $Y$ que no es explicada por $X$.
    -   **Varianza Total de** $Y$: Es la varianza de los valores observados de $Y$.

## Coeficiente de Determinaci√≥n $R^2$ y suma de cuadrados {.smaller background-color="white"}

Otra forma de expresar el $R^2$ es la siguiente:

-   Se calcula como: $$
    R^2 = \frac{\text{SSR}}{\text{SST}} = 1 - \frac{\text{SSE}}{\text{SST}}
    $$ Donde:
    -   **SSR (Sum of Squares Regression):** Es la suma de los cuadrados de las diferencias entre los valores predichos $\hat{Y}$ y la media $\bar{Y}$.
    -   **SSE (Sum of Squares Error):** Es la suma de los cuadrados de los residuos, es decir, la parte de $Y$ que no es explicada por $X$.
    -   **SST (Sum of Squares Total):** Es la suma de los cuadrados de las diferencias entre los valores observados $Y$ y la media $\bar{Y}$.

## Descomposici√≥n de $Y$: Relaci√≥n con $R^2$ {.smaller background-color="white"}

-   Cuando descomponemos $Y_i$ en sus componentes, tenemos:
    $$
    Y_i = \bar{Y} + (\hat{Y}_i ‚àí \bar{Y}) + (Y_i ‚àí \hat{Y}_i)
    $$

-   $Y_i$: Valor observado.

-   $\hat{Y}_i ‚àí \bar{Y}$: Componente explicado por $X$.

-   $Y_i ‚àí \hat{Y}_i$: Residuos o componente no explicado por $X$.

-   **Varianza Residual**: Corresponde a la varianza del componente no explicado $Y_i ‚àí \hat{Y}_i$.

## $R^2$ como Proporci√≥n Explicada {.smaller background-color="white"}

-   $R^2$ indica cu√°nta de la varianza total de $Y$ es explicada por el modelo.
-   Un $R^2$ cercano a 1 sugiere que la mayor parte de la varianza de $Y$ es explicada por $X$.
-   Un $R^2$ cercano a 0 sugiere que el modelo no explica bien la varianza de $Y$, y la varianza residual es alta.

**Interpretaci√≥n Pr√°ctica de** $R^2$: - Un $R^2$ de 0.18 indica que el 19% de la varianza en $Y$ es explicada por $X$, mientras que el 81% restante es debido a factores no capturados por el modelo (varianza residual).

## Observacions at√≠picas y observaciones influyentes en regresi√≥n {.smaller background-color="white"}

-   Una observaci√≥n **at√≠pica** es aqu√©lla que queda separada de las restantes observaciones.\
-   Una observaci√≥n es **influyente** con relaci√≥n a un c√°lculo estad√≠stico si al eliminarla cambia el resultado del c√°lculo. En regresi√≥n m√≠nimocuadr√°tica, las observaciones at√≠picas en la direcci√≥n del eje de las abscisas son, en general, observaciones influyentes.

## Precauciones con la Correlaci√≥n y la Regresi√≥n {.smaller background-color="white"}

La correlaci√≥n y la regresi√≥n son herramientas poderosas, pero tienen limitaciones.

Precauciones:

-   *Extrapolaci√≥n:* Evita predecir fuera del rango de valores utilizados para calcular la recta de regresi√≥n, ya que esto puede llevar a resultados no fiables.\
-   *Medias:* Los estudios que usan medias pueden mostrar correlaciones demasiado altas cuando se aplican a individuos.\
-   *Variables Latentes:* Considera la posibilidad de variables no medidas que pueden estar influyendo en la relaci√≥n observada.\
-   *Asociaci√≥n No Implica Causalidad:* Una fuerte asociaci√≥n entre dos variables no garantiza una relaci√≥n causa-efecto.
