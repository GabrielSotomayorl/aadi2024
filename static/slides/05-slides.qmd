---
title: "Clase 5 <br> Regresi√≥n Lineal M√∫ltiple I"
subtitle: "An√°lisis Avanzado de Datos"
author: "Gabriel Sotomayor"
format: 
  revealjs:
    theme: [dark, custom.scss]
    slide-number: true
    auto-fit: true
    logo: images/logo.png
editor: visual
---

## Revisi√≥n de la prueba {.smaller background-color="white"}

1)  ¬øCu√°l de las siguientes afirmaciones sobre la correlaci√≥n es FALSA?

```{=html}
<!-- -->
```
a)  La correlaci√≥n es sim√©trica, lo que significa que no distingue entre variables explicativas y de respuesta.
b)  Valores cercanos a +1 o -1 indican una mayor fuerza de la relaci√≥n.
c)  Un valor de correlaci√≥n cercano a 0 indica una relaci√≥n lineal d√©bil o nula entre las variables.
d)  La correlaci√≥n siempre describe con precisi√≥n la relaci√≥n entre variables, incluso si la relaci√≥n no es lineal.
e)  El valor de la correlaci√≥n no cambia si se modifican las unidades de medida de las variables.

## Revisi√≥n de la prueba {.smaller background-color="white"}

1)  ¬øCu√°l de las siguientes afirmaciones sobre la correlaci√≥n es FALSA?

```{=html}
<!-- -->
```
a)  La correlaci√≥n es sim√©trica, lo que significa que no distingue entre variables explicativas y de respuesta.
b)  Valores cercanos a +1 o -1 indican una mayor fuerza de la relaci√≥n.
c)  Un valor de correlaci√≥n cercano a 0 indica una relaci√≥n lineal d√©bil o nula entre las variables.
d)  **La correlaci√≥n siempre describe con precisi√≥n la relaci√≥n entre variables, incluso si la relaci√≥n no es lineal.**
e)  El valor de la correlaci√≥n no cambia si se modifican las unidades de medida de las variables.

## Revisi√≥n de la prueba {.smaller background-color="white"}

2)  El R¬≤ o coeficiente de determinaci√≥n da cuenta de:

```{=html}
<!-- -->
```
a)  La proporci√≥n de la variabilidad total en la variable dependiente que el modelo NO puede explicar.
b)  La relaci√≥n causal entre una variable independiente y una variable dependiente.
c)  La precisi√≥n con la que el modelo puede predecir valores futuros de la variable independiente.
d)  La diferencia entre los valores observados y los valores predichos en un modelo de regresi√≥n.
e)  El porcentaje de la varianza en la variable dependiente que puede ser explicado por la varianza en la variable independiente.

## Revisi√≥n de la prueba {.smaller background-color="white"}

2)  El R¬≤ o coeficiente de determinaci√≥n da cuenta de:

```{=html}
<!-- -->
```
a)  La proporci√≥n de la variabilidad total en la variable dependiente que el modelo NO puede explicar.
b)  La relaci√≥n causal entre una variable independiente y una variable dependiente.
c)  La precisi√≥n con la que el modelo puede predecir valores futuros de la variable independiente.
d)  La diferencia entre los valores observados y los valores predichos en un modelo de regresi√≥n.
e)  **El porcentaje de la varianza en la variable dependiente que puede ser explicado por la varianza en la variable independiente.**

## Revisi√≥n de la prueba {.smaller background-color="white"}

3)  ¬øCu√°l es la interpretaci√≥n correcta de un coeficiente beta (ùëè) o pendiente en un modelo de regresi√≥n lineal simple?

```{=html}
<!-- -->
```
a)  La cantidad de varianza en la variable independiente (X) que puede ser explicada por la variable dependiente (Y).
b)  El valor promedio de la variable dependiente (Y) cuando la variable independiente (X) es cero.
c)  El grado de asociaci√≥n entre la variable dependiente (Y) y la variable independiente (X).
d)  La cantidad de cambio en la variable dependiente (Y) cuando la variable independiente (X) aumenta en una unidad.
e)  La fuerza de la relaci√≥n entre dos variables, similar al coeficiente de correlaci√≥n.

## Revisi√≥n de la prueba {.smaller background-color="white"}

3)  ¬øCu√°l es la interpretaci√≥n correcta de un coeficiente beta (ùëè) o pendiente en un modelo de regresi√≥n lineal simple?

```{=html}
<!-- -->
```
a)  La cantidad de varianza en la variable independiente (X) que puede ser explicada por la variable dependiente (Y).
b)  El valor promedio de la variable dependiente (Y) cuando la variable independiente (X) es cero.
c)  El grado de asociaci√≥n entre la variable dependiente (Y) y la variable independiente (X).
d)  **La cantidad de cambio en la variable dependiente (Y) cuando la variable independiente (X) aumenta en una unidad.**
e)  La fuerza de la relaci√≥n entre dos variables, similar al coeficiente de correlaci√≥n.

## Revisi√≥n de la prueba {.smaller background-color="white"}

4)  ¬øCu√°l es la interpretaci√≥n correcta del intercepto (a) en un modelo de regresi√≥n lineal simple?

```{=html}
<!-- -->
```
a)  El cambio esperado en la variable dependiente (Y) por cada unidad de aumento en la variable independiente
b)  El valor esperado de la variable independiente (X) cuando la variable dependiente (Y) es cero.
c)  El valor esperado de la variable dependiente (Y) cuando la variable independiente (X) es cero.
d)  La cantidad de variaci√≥n en la variable dependiente (Y) que puede ser explicada por el modelo.
e)  La pendiente de la l√≠nea de regresi√≥n.

## Revisi√≥n de la prueba {.smaller background-color="white"}

4)  ¬øCu√°l es la interpretaci√≥n correcta del intercepto (a) en un modelo de regresi√≥n lineal simple?

```{=html}
<!-- -->
```
a)  El cambio esperado en la variable dependiente (Y) por cada unidad de aumento en la variable independiente
b)  El valor esperado de la variable independiente (X) cuando la variable dependiente (Y) es cero.
c)  **El valor esperado de la variable dependiente (Y) cuando la variable independiente (X) es cero.**
d)  La cantidad de variaci√≥n en la variable dependiente (Y) que puede ser explicada por el modelo.
e)  La pendiente de la l√≠nea de regresi√≥n.

## Revisi√≥n de la prueba {.smaller background-color="white"}

5)  ¬øQu√© es un residuo en un modelo de regresi√≥n lineal?

```{=html}
<!-- -->
```
a)  Es la diferencia entre el valor predicho y el valor promedio de la variable dependiente (Y).
b)  La suma de los errores cometidos por el modelo al predecir los valores de la variable independiente (X).
c)  La diferencia entre el valor observado de la variable dependiente (Y) y el valor predicho (≈∑) por la recta de regresi√≥n.
d)  El valor promedio de la variable dependiente (Y) en el modelo.
e)  El valor predicho de la variable independiente (X) en el modelo.

## Revisi√≥n de la prueba {.smaller background-color="white"}

5)  ¬øQu√© es un residuo en un modelo de regresi√≥n lineal?

```{=html}
<!-- -->
```
a)  Es la diferencia entre el valor predicho y el valor promedio de la variable dependiente (Y).
b)  La suma de los errores cometidos por el modelo al predecir los valores de la variable independiente (X).
c)  **La diferencia entre el valor observado de la variable dependiente (Y) y el valor predicho (≈∑) por la recta de regresi√≥n.**
d)  El valor promedio de la variable dependiente (Y) en el modelo.
e)  El valor predicho de la variable independiente (X) en el modelo.

## Revisi√≥n de la prueba {.smaller background-color="white"}

6)  En el texto Esser, este plantea (al menos) cinco cr√≠ticas a el enfoque que llama ‚ÄúSociolog√≠a de las Variables‚Äù el cual define como un enfoque en que se identifica una variable dependiente (explanandum) y se propone un conjunto de variables independientes (explanans) que podr√≠an influir en ella, y la explicaci√≥n se considera lograda cuando se puede atribuir la varianza de la variable dependiente a los efectos de las variables independientes. **Mencione y explique dos de las cr√≠ticas planteadas por Esser a este enfoque.**

## Problemas de la Sociolog√≠a de las Variables {.smaller background-color="white"}

**Incompletitud:** Las relaciones entre variables establecidas en un contexto pueden no ser aplicables en otros, revelando la falta de leyes sociol√≥gicas generales y estables. La SV, al intentar explicar fen√≥menos sociales, frecuentemente se queda en explicaciones ad hoc, lo que limita su alcance y efectividad.

**Significado Variable:** Las variables estructurales pueden tener significados diferentes seg√∫n el contexto cultural o social. Este problema de equivalencia funcional implica que las mismas variables no siempre tienen el mismo impacto en diferentes escenarios, lo que dificulta la creaci√≥n de explicaciones universales.

**Reduccionismo:** Al reducir fen√≥menos sociales a simples relaciones entre variables, la SV pierde de vista la complejidad de las decisiones individuales y colectivas, y c√≥mo estas influyen en los resultados sociales.

## Problemas de la Sociolog√≠a de las Variables {.smaller background-color="white"}

**Interdependencia:** Las estructuras sociales son procesos din√°micos donde las interacciones entre individuos y procesos son complejas. La SV no aborda adecuadamente c√≥mo estas interdependencias afectan los resultados sociales, limitando la capacidad de la SV para explicar fen√≥menos complejos.

**Falta de Sentido:** La SV ignora el sentido subjetivo de las acciones individuales, centr√°ndose solo en relaciones entre variables. Esto deja de lado la dimensi√≥n interpretativa crucial para una explicaci√≥n sociol√≥gica completa, que considera las decisiones conscientes de los individuos.

## Revisi√≥n de la prueba {.smaller background-color="white"}

**a) Interpretaci√≥n de los coeficientes del modelo (intercepto y beta de regresi√≥n):**

Intercepto (-272.321): El valor del intercepto indica que, cuando una persona tiene cero a√±os de escolaridad, su ingreso esperado ser√≠a -272.321 pesos. Aunque este valor no tiene sentido en t√©rminos pr√°cticos (ya que no es posible tener ingresos negativos), sirve como punto de referencia en el modelo.

Beta para A√±os de escolaridad (77.114): Este coeficiente indica que, por cada a√±o adicional de escolaridad, el ingreso esperado aumenta en 77.114 pesos. En otras palabras, la educaci√≥n tiene un impacto positivo en los ingresos laborales.

**b) Interpretaci√≥n y evaluaci√≥n del ajuste del modelo (R¬≤):**

R¬≤ = 0.13: El valor de R¬≤ nos indica que el 13% de la variabilidad en los ingresos laborales puede ser explicada por los a√±os de escolaridad. Aunque el modelo tiene una relaci√≥n positiva entre la educaci√≥n y los ingresos, el bajo valor de R¬≤ sugiere que hay muchos otros factores que tambi√©n influyen en los ingresos y que no est√°n considerados en este modelo.

## [Evaluaciones]{style="color:#FFFFFF;"} {.dark-background}

**Tarea 2: 9 de octubre**\
- Regresi√≥n lineal m√∫ltiple

**Informe 1: 30 de Octubre**\
- Regresi√≥n lineal m√∫ltiple o regresi√≥n log√≠stica

# Recordatorio de RLS

## Recta de Regresi√≥n M√≠nimo-Cuadr√°tica {.smaller background-color="white"}

La regresi√≥n lineal simple se utiliza para describir la relaci√≥n entre dos variables, una independiente (explicativa) y una dependiente (respuesta), mediante una recta de regresi√≥n.

#### F√≥rmula General {background-color="white"}

La recta de regresi√≥n se expresa como: $$
\hat{y} = a + bx
$$

**Pendiente ùëè:** Indica el cambio promedio en la variable respuestaùë¶por cada unidad de cambio en la variable explicativa ùë•.\
**Ordenada en el origen ùëé:** Representa el valor predicho deùë¶cuando ùë•= 0. S√≥lo tiene significado estad√≠stico cuando x toma valores cercanos a 0.

## Coeficiente de Determinaci√≥n $R^2$ y Varianza Residual {.smaller background-color="white"}

**¬øQu√© es** $R^2$? - $R^2$, conocido como el coeficiente de determinaci√≥n, es una medida estad√≠stica que indica la proporci√≥n de la varianza en la variable dependiente $Y$ que es explicada por la variable independiente $X$ en un modelo de regresi√≥n.

-   Se calcula como: $$
    R^2 = 1 - \frac{\text{Varianza Residual}}{\text{Varianza Total de } Y}
    $$ Donde:
    -   **Varianza Residual:** Es la varianza de los residuos, es decir, la parte de $Y$ que no es explicada por $X$.
    -   **Varianza Total de** $Y$: Es la varianza de los valores observados de $Y$.

## $R^2$ como Proporci√≥n Explicada {.smaller background-color="white"}

-   $R^2$ indica cu√°nta de la varianza total de $Y$ es explicada por el modelo.
-   Un $R^2$ cercano a 1 sugiere que la mayor parte de la varianza de $Y$ es explicada por $X$.
-   Un $R^2$ cercano a 0 sugiere que el modelo no explica bien la varianza de $Y$, y la varianza residual es alta.

**Interpretaci√≥n Pr√°ctica de** $R^2$: - Un $R^2$ de 0.18 indica que el 19% de la varianza en $Y$ es explicada por $X$, mientras que el 81% restante es debido a factores no capturados por el modelo (varianza residual).

## Objetivo de la sesi√≥n {.center background-color="white"}

Introducir el concepto de control estad√≠stico y el uso de regresi√≥n lineal m√∫ltiple.

## El problema del control estad√≠stico. {.smaller background-color="white"}

El control estad√≠stico consiste en ajustar los an√°lisis para "controlar" el efecto de otras variables (covariadas) que podr√≠an estar influyendo en la relaci√≥n entre las variables de inter√©s.

Ejemplo: En un estudio sobre diferencias salariales entre hombres y mujeres, las covariadas pueden incluir a√±os de empleo o nivel educativo.

## Distintas formas de control {.smaller background-color="white"}

Podemos controlar por otras variables que pueden influir en nuestros resultados a partir del dise√±o de nuestra investigaci√≥n: A partir de asignaci√≥n aleatoria en un experimento.

Por otro lado podemos controlar estad√≠sticamente: Control Ajuste matem√°tico que no requiere manipulaci√≥n directa de datos o exclusi√≥n de casos. Es lo que comunmente tendremos que hacer en el contexto de estudios observacionales.

## Ventajas del control estad√≠stico {.smaller background-color="white"}

-   No se manipulan participantes ni condiciones.
-   No requiere excluir datos.
-   Permite "mantener constantes" ciertas variables para observar el efecto "puro" de la variable independiente.
-   Limitaciones: Requiere medici√≥n precisa de las covariadas y puede haber desacuerdo sobre qu√© variables controlar.

# Regresi√≥n lineal m√∫ltiple

## Introducci√≥n al Modelo de Regresi√≥n M√∫ltiple {.smaller background-color="white"}

Un modelo de regresi√≥n m√∫ltiple examina la relaci√≥n entre una **variable dependiente** y **varias variables independientes o predictores**.\
La regresi√≥n m√∫ltiple permite **controlar otras variables** mientras se eval√∫a el efecto de una variable predictora espec√≠fica.\
Ejemplo: Si estudiamos la relaci√≥n entre el ejercicio y la p√©rdida de peso, tambi√©n podemos controlar la cantidad de alimentos consumidos para aislar su efecto.

Ecuaci√≥n b√°sica del modelo:

$$ Y = b_0 + b_1 X_1 + b_2 X_2 + \dots + b_k X_k + \epsilon $$ Donde $Y$ es la variable dependiente, $b_0$ es la constante o intercepto, $X_1, X_2, \dots X_k$ son las variables independientes, $b_1, b_2, \dots b_k$ son los coeficientes de regresi√≥n, y $\epsilon$ es el error.

## Asociaci√≥n Parcial {.smaller background-color="white"}

La asociaci√≥n parcial mide la relaci√≥n entre dos variables manteniendo constantes otras variables.

Ejemplo: En un estudio sobre p√©rdida de peso, podemos medir la relaci√≥n entre la ingesta de alimentos y la p√©rdida de peso, controlando la cantidad de ejercicio realizado.

Veamos c√≥mo las asociaciones cambian al controlar variables adicionales.

## Ejemplo {.smaller background-color="white"}

| ID           | Frecuencia de ejercicio (horas semanales promedio) $X_1$ | Ingesta diaria promedio de alimentos (100s de calor√≠as por encima del recomendado) $X_2$ | P√©rdida de peso semanal promedio (100s de gramos) $Y$ |
|--------------|----------------|----------------------------|---------------|
| 1            | 0                                                        | 2                                                                                        | 6                                                     |
| 2            | 0                                                        | 4                                                                                        | 2                                                     |
| 3            | 0                                                        | 6                                                                                        | 4                                                     |
| 4            | 2                                                        | 2                                                                                        | 8                                                     |
| 5            | 2                                                        | 4                                                                                        | 9                                                     |
| 6            | 2                                                        | 6                                                                                        | 8                                                     |
| 7            | 2                                                        | 8                                                                                        | 5                                                     |
| 8            | 4                                                        | 4                                                                                        | 11                                                    |
| 9            | 4                                                        | 6                                                                                        | 13                                                    |
| 10           | 4                                                        | 8                                                                                        | 9                                                     |
| **Promedio** | **2**                                                    | **5**                                                                                    | **7.5**                                               |

## Correlaci√≥n simple entre ejercicio y perdida de peso {.smaller background-color="white"}

La correlaci√≥n entre ejercicio y p√©rdida de peso es positiva ($r_{X_1Y} = 0.864$).

**Interpretaci√≥n:** Los participantes que hacen m√°s ejercicio tienden a perder m√°s peso.

**Conclusi√≥n:** M√°s ejercicio est√° asociado con mayor p√©rdida de peso.

## Correlaci√≥n simple entre ejercicio y perdida de peso {.smaller background-color="white"}

```{r}
library(tidyverse)
data <- data.frame(
  id = 1:10,
  ej = c(0, 0, 0, 2, 2, 2, 2, 4, 4, 4),  # Frecuencia de ejercicio (horas semanales promedio)
  cal = c(2, 4, 6, 2, 4, 6, 8, 4, 6, 8),  # Ingesta diaria promedio de alimentos (100s de calor√≠as sobre el recomendado)
  ppeso  = c(6, 2, 4, 8, 9, 8, 5, 11, 13, 9) # P√©rdida de peso semanal promedio (100s de gramos)
)

ggplot(data, aes(x = ej, y = ppeso)) +
  geom_point() +  # Agregar los puntos de dispersi√≥n
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Agregar la l√≠nea de regresi√≥n
  labs(
    title = "Relaci√≥n entre ejercicio y p√©rdida de peso",
    x = "Horas de ejercicio semanales (X1)",
    y = "P√©rdida de peso (100s de gramos, Y)"
  ) +
  theme_minimal()

```

## Correlaci√≥n simple entre consumo de comida y P√©rdida de Peso {.smaller background-color="white"}

La correlaci√≥n entre ingesta de alimentos y p√©rdida de peso es peque√±a y positiva ($r_{X_2Y} = 0.047$).

**Contraintuitivo:** Se esperar√≠a que comer m√°s implique perder menos peso, pero los datos sugieren lo contrario.

**Pregunta:** ¬øQu√© est√° pasando aqu√≠? El ejercicio puede estar ocultando la verdadera relaci√≥n entre comida y p√©rdida de peso.

## Correlaci√≥n simple entre consumo de comida y P√©rdida de Peso {.smaller background-color="white"}

```{r}
# Gr√°fico de dispersi√≥n entre ingesta de alimentos y p√©rdida de peso con l√≠nea de regresi√≥n
ggplot(data, aes(x = cal, y = ppeso)) +
  geom_point() +  # Agregar los puntos de dispersi√≥n
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Agregar la l√≠nea de regresi√≥n
  labs(
    title = "Relaci√≥n entre ingesta de alimentos y p√©rdida de peso",
    x = "Ingesta diaria de alimentos (100s de calor√≠as, X2)",
    y = "P√©rdida de peso (100s de gramos, Y)"
  ) +
  theme_minimal()
```

## Relaci√≥n entre consumo de comida y perdida de peso, controlando por ejercicio {background-color="white"}

```{r}
# Convertir las horas de ejercicio en un factor
data$ej_factor <- as.factor(data$ej)

# Gr√°fico de dispersi√≥n con diferentes colores para las horas de ejercicio y l√≠neas de regresi√≥n separadas
ggplot(data, aes(x = cal, y = ppeso, color = ej_factor)) +
  geom_point() +  # Agregar los puntos de dispersi√≥n
  geom_smooth(method = "lm", se = FALSE) +  # Agregar la l√≠nea de regresi√≥n separada por cada nivel de ejercicio
  labs(
    title = "Relaci√≥n entre ingesta de alimentos y p√©rdida de peso",
    x = "Ingesta diaria de alimentos (100s de calor√≠as, X2)",
    y = "P√©rdida de peso (100s de gramos, Y)",
    color = "Horas de ejercicio (X1)"
  ) +
  theme_minimal()
```

## Relaci√≥n entre consumo de comida y perdida de peso, controlando por ejercicio {background-color="white"}

Al controlar el nivel de ejercicio, podemos ver la verdadera relaci√≥n entre la ingesta de alimentos y la p√©rdida de peso.

Comer m√°s realmente se asocia con perder menos peso, pero este efecto estaba oculto debido al impacto del ejercicio.

## Modelo de Regresi√≥n M√∫ltiple {.smaller background-color="white"}

**Ecuaci√≥n:**

$$
Y = 6 + 2X_1 - 0.5X_2
$$

**Interpretaci√≥n:**

-   $b_0 = 6$: La p√©rdida de peso esperada (en cientos de gramos) para alguien que no hace ejercicio ni consume calor√≠as extra.
-   $b_1 = 2$: Por cada hora adicional de ejercicio semanal, se espera una p√©rdida de peso adicional de 200 gramos, manteniendo constante la ingesta de alimentos.
-   $b_2 = -0.5$: Por cada 100 calor√≠as extra consumidas, se espera perder 50 gramos menos, manteniendo constante el ejercicio.

El modelo permite aislar los efectos de cada predictor, controlando por los dem√°s.

## Modelo de Regresi√≥n M√∫ltiple {.smaller background-color="white"}

![](img/05/plano.jpg)

## Ajuste del modelo y residuos {.smaller background-color="white"}

**M√©todo de m√≠nimos cuadrados:**
- El ajuste del modelo se realiza utilizando el m√©todo de m√≠nimos cuadrados.
- Este m√©todo busca minimizar la suma de los residuos al cuadrado, que son las diferencias entre los valores observados ($Y$) y los valores predichos ($\hat{Y}$) por el modelo.
  
**Minimizaci√≥n de los residuos:**
- Los residuos ($e_i$) se calculan como la diferencia entre el valor observado y el valor predicho:
  
  $$ e_i = Y_i - \hat{Y}_i $$

- El objetivo del modelo es minimizar la suma de los residuos al cuadrado:
  
  $$ \sum (Y_i - \hat{Y}_i)^2 $$

El uso de m√≠nimos cuadrados garantiza que el plano de regresi√≥n ajustado se acerque lo m√°s posible a los puntos de datos observados.
