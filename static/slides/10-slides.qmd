---
title: "Clase 10 <br> Diagnosticos y supuestos de modelos de regresión"
subtitle: "Análisis Avanzado de Datos"
author: "Gabriel Sotomayor"
format: 
  revealjs:
    theme: [dark, custom.scss]
    slide-number: true
    auto-fit: true
    logo: images/logo.png
editor: visual
---

# Diagnosticos de modelos de regresión


## Introducción a los Diagnósticos de Regresión {.smaller background-color="white"}

Este capítulo aborda el tema del diagnóstico de regresión, introduciendo conceptos como **leverage**, **distancia**, e **influencia** para identificar casos irregulares en un análisis de regresión. 

Los objetivos son:

- Detectar errores de digitación (o similares).
- Identificar casos irregulares que puedan distorsionar el análisis.
- Comprobar si se cumplen los supuestos de la regresión lineal.

**¿Por qué es importante?** Los diagnósticos nos permiten entender mejor la relación entre nuestras variables, asegurándonos de que no hay observaciones que estén afectando desproporcionadamente nuestros resultados. Además, nos ayuda a cumplir con los supuestos fundamentales de la regresión lineal, lo que asegura la validez de las inferencias.

## ¿Por Qué Son Importantes los Diagnósticos?  {.smaller background-color="white"}

- **Errores de Datos**: Errores comunes como ingresar incorrectamente la edad, omitir información, o respuestas inusuales pueden afectar significativamente el análisis y llevarnos a conclusiones erróneas.
- **Identificación de Casos Atípicos**: Un caso inusual podría modificar de manera significativa los resultados de un estudio, llevando a coeficientes sesgados o modelos que no representan correctamente la relación entre las variables.
- **Garantizar la Validez del Modelo**: Los diagnósticos estadísticos permiten detectar estas irregularidades antes de interpretar los resultados, asegurando que nuestras conclusiones sean válidas y robustas.

## Conceptos Clave  {.smaller background-color="white"}

- **Leverage**: Identifica cuán alejada está una observación de las demás en los predictores (X). Observaciones con un valor de leverage alto tienen el potencial de influir desproporcionadamente en la regresión, particularmente cuando no siguen el patrón general de los datos.
- **Distancia**: Cuantifica la distancia vertical entre el valor observado (Y) y el ajustado ($\hat{Y}$). Los residuos representan la diferencia entre lo observado y lo predicho; un residuo grande puede indicar un outlier.
- **Influencia**: Representa el efecto que tiene un caso en el ajuste general de la regresión. Las observaciones influyentes pueden cambiar significativamente los coeficientes del modelo.

Es importante entender la interacción entre estos conceptos, ya que un punto con alto leverage podría no ser influyente si su residuo es bajo, pero un caso con alto leverage y gran residuo será altamente influyente.

## Tipos de Casos Extremos  {.smaller background-color="white"}

- **Puntos de Leverage (Leverage Points)**: Casos que son extremos en sus valores de los predictores. Estos puntos se encuentran alejados del centro de la distribución de X y tienen un alto potencial de influir en el modelo.
- **Outliers**: Casos con valores de respuesta (Y) muy alejados de los valores predichos. Los outliers pueden indicar que ciertos supuestos no se cumplen o que hay algo especial en esos datos que los hace diferentes del resto.
- **Casos Influyentes**: Observaciones que tienen un gran impacto en los coeficientes del modelo y, por tanto, en los resultados generales. Estos se identifican mejor con métricas como la distancia de Cook.

## Métodos para Detectar Casos Irregulares  {.smaller background-color="white"}

### Leverage

Los valores de leverage altos indican un patrón de valores de predictores que difiere considerablemente de los demás casos. 

- **Medida**: El **Leverage** se calcula usando la matriz de diseño y se relaciona con la distancia de Mahalanobis ($MD_i$). Valores altos de leverage indican que una observación está lejos del centroide de los datos en el espacio de los predictores.
- **Uso**: Identificar observaciones extremas en los valores de los predictores, que podrían tener una gran capacidad de influenciar el ajuste del modelo.  

## Métodos para Detectar Casos Irregulares  {.smaller background-color="white"}  

### Distancia: Residuos

- Los **residuos** ($e_i = Y_i - \hat{Y}_i$) miden la discrepancia entre el valor observado y el valor ajustado.
- Las observaciones con **residuos grandes** podrían ser outliers o sugerir un mal ajuste del modelo, indicando la necesidad de revisar los supuestos o los datos.
- **Residuos Estandarizado**: Residuos divididos por su desviación estándar, lo cual permite comparar entre observaciones y detectar outliers de manera más precisa.  

## Métodos para Detectar Casos Irregulares  {.smaller background-color="white"}  
  
### Influencia: Cook's Distance

- La **distancia de Cook** ($Cook_i$) mide cuánto cambiarian los valores predichos por la regresión si se elimina un caso específico. Combina la información de leverage y de los residuos.
- **Influencia significativa**: Un caso tiene alta influencia si tiene **alta leverage y alta distancia**. Se recomienda comparar el valor de Cook con un valor crítico, generalmente **4/n**.

## Un ejemplo clásico: El Cuarteto de Anscombe {.smaller background-color="white"}  

```{r}
# Cargar paquetes necesarios
library(ggplot2)
library(gridExtra)

# Datos del Cuarteto de Anscombe
anscombe <- data.frame(
  x1 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
  y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),
  x2 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
  y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74),
  x3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
  y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),
  x4 = c(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 19),
  y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 5.56, 7.91, 6.89, 12.5)
)

# Personalización general para un tema más limpio y estilizado
custom_theme <- theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, color = "#4A4A4A"),
    axis.title = element_text(face = "bold", size = 14, color = "#4A4A4A"),
    axis.text = element_text(color = "#4A4A4A"),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank()
  )

# Función para crear cada gráfico con la ecuación de regresión
plot_anscombe <- function(x, y, title) {
  modelo <- lm(y ~ x)
  coef <- coef(modelo)
  eq <- paste0("Y = ", round(coef[1], 2), " + ", round(coef[2], 2), " * X")
  
  ggplot(data.frame(x, y), aes(x = x, y = y)) +
    geom_point(color = "#2C7FB8", size = 4, alpha = 0.8) +
    geom_smooth(method = "lm", se = FALSE, color = "#EF6548", linetype = "dashed", size = 1) +
    labs(title = title, x = "X", y = "Y") +
    annotate("text", x = min(x), y = max(y) - 0.5, label = eq, hjust = 0, size = 5, color = "#4A4A4A") +
    custom_theme
}

# Crear los gráficos individuales
plot1 <- plot_anscombe(anscombe$x1, anscombe$y1, "Dataset 1")
plot2 <- plot_anscombe(anscombe$x2, anscombe$y2, "Dataset 2")
plot3 <- plot_anscombe(anscombe$x3, anscombe$y3, "Dataset 3")
plot4 <- plot_anscombe(anscombe$x4, anscombe$y4, "Dataset 4")

# Organizar los gráficos en una cuadrícula
grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
```

## Tratamiento de Irregularidades {.smaller background-color="white"}


- **Corrección**: Modificar errores clericales o valores extremos que se puedan justificar, como valores atípicos debidos a errores de entrada de datos.
- **Transformación**: Si hay problemas de homocedasticidad o falta de normalidad, transformar la variable dependiente puede ser útil.
- **Eliminación de casos**: Solo se debe eliminar una observación si se puede justificar adecuadamente, como en el caso de errores de medición claramente identificados.

# Supuestos

##  Homocedasticidad de los errores {.smaller background-color="white"}

- **Homocedasticidad** significa que los **residuales** tienen **varianza constante** para todos los valores de $X$. Si no se cumple, la regresión pierde eficiencia y las inferencias pueden ser incorrectas.
- **Detección**: Utilizar gráficos de **residuos vs valores predichos** para detectar patrones que indiquen varianza no constante (ej. un patrón en forma de embudo).
- **Prueba de Breusch-Pagan**: Esta prueba estadística evalúa si la varianza de los residuos depende de los valores de los predictores. Un p-valor bajo indica violación de homocedasticidad.
- El escenario contrario es heterocedasticidad de los errores.   

**Solución: Modelos con errores estándares robustos**


##  Homocedasticidad de los errores {.smaller background-color="white"}

![](img/04/residuos.jpg) {.smaller background-color="white"}




## Normalidad en la distribución de los residuos {.smaller background-color="white"}

Los residuos en torno a los valores estimados de Y se distribuyen normalmente para que las infernecias sean válidas.  
- Si los residuos se distribuyen normalmente, quiere decir que la mayor parte de los residuos se encuentran en torno a 0 (es decir, son valores que se alejan poco del valor observado).  
- A su vez, son cada vez menos los residuos a medida que estos valores son mayores en términos absolutos.

- **Detección**: Verificarlo con un **Q-Q plot** y aplicar la **prueba de Shapiro-Wilk**. La falta de normalidad puede llevar a estimaciones sesgadas de los coeficientes.

## Normalidad en la distribución de los residuos {.smaller background-color="white"}
```{r}

# Configuración de datos simulados con un tamaño de muestra más grande para mejorar la normalidad
set.seed(123)
n <- 500  # Incremento en el número de observaciones para una mayor precisión
x <- rnorm(n)
y_normal <- 3 + 2 * x + rnorm(n, mean = 0, sd = 1)  # Modelo con errores normales ajustados
y_non_normal <- 3 + 2 * x + rt(n, df = 3)  # Modelo con errores no normales (t-Student con df bajos)

# Ajustar los modelos de regresión
model_normal <- lm(y_normal ~ x)
model_non_normal <- lm(y_non_normal ~ x)

# Extraer residuos estandarizados
residuals_normal <- rstandard(model_normal)
residuals_non_normal <- rstandard(model_non_normal)

# Tema personalizado
custom_theme <- theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, color = "#4A4A4A"),
    axis.title = element_text(face = "bold", size = 14, color = "#4A4A4A"),
    axis.text = element_text(color = "#4A4A4A"),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank()
  )

# Función para gráficos de histograma
plot_histogram <- function(residuals, title) {
  ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
    geom_histogram(aes(y = ..density..), bins = 20, color = "black", fill = "#ADD8E6") +
    geom_density(color = "#1E90FF", size = 1) +
    labs(title = title, x = "Residuos estandarizados", y = "Densidad") +
    custom_theme
}

# Función para gráficos Q-Q plot
plot_qq <- function(residuals, title) {
  ggplot(data.frame(sample = residuals), aes(sample = sample)) +
    stat_qq(color = "#2C7FB8", size = 1.5) +
    stat_qq_line(color = "#EF6548", linetype = "dashed", size = 1) +
    labs(title = title, x = "Cuantiles teóricos", y = "Cuantiles de los residuos") +
    custom_theme
}

# Crear los gráficos individuales
p1 <- plot_histogram(residuals_normal, "Histograma: Errores Normales Ajustados")
p2 <- plot_qq(residuals_normal, "Q-Q Plot: Errores Normales Ajustados")
p3 <- plot_histogram(residuals_non_normal, "Histograma: Errores No Normales")
p4 <- plot_qq(residuals_non_normal, "Q-Q Plot: Errores No Normales")

# Organizar los gráficos en una cuadrícula 2x2
grid.arrange(p1, p2, p3, p4, ncol = 2)


```


## Relación lineal {.smaller background-color="white"}
- Se asume que la relación entre cada predictor y la respuesta es lineal (porque eso es lo que estamos ajustando en una regresión lineal).
- Si la relación es aproximadamente lineal, tiene sentido usar modelos de regresión lineal.
- **Detección**: Utilizar gráficos de **residupos vs predicciones** para verificar la relación lineal.     También se puede evaluar en base a diagramas de dispersión (si el n es pequeño).
- Una correlación alta entre las variables es indicación de que la relación es lineal.
- Si la relación no es lineal, considerar utilizar otro tipo de regresión o transformar variables (por ejemplo, ver efectos de variables al cuadrado).

## Relación lineal {.smaller background-color="white"} 
```{r}

# Tema personalizado para los gráficos
custom_theme <- theme_minimal(base_size = 10) +
  theme(
    plot.title = element_text(face = "bold", size = 12, color = "#4A4A4A"),
    axis.title = element_text(face = "bold", size = 10, color = "#4A4A4A"),
    axis.text = element_text(color = "#4A4A4A"),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank()
  )

# Configuración de datos simulados
set.seed(1233)
n <- 100
x <- rnorm(n, 0, 1)

# Escenario 1: Relación Lineal
y_linear <- 3 + 2 * x + rnorm(n, mean = 0, sd = 0.5)  # Relación lineal con menor error
model_linear <- lm(y_linear ~ x)
data_linear <- data.frame(x, y_linear, Fitted = model_linear$fitted.values, Residuals = rstandard(model_linear))

# Escenario 2: Relación Cuadrática
y_quadratic <- 3 + 2 * x + 1 * x^2 + rnorm(n, mean = 0, sd = 0.5)  # Relación cuadrática con menor error
model_quadratic <- lm(y_quadratic ~ x)
data_quadratic <- data.frame(x, y_quadratic, Fitted = model_quadratic$fitted.values, Residuals = rstandard(model_quadratic))

# Gráfico 1: Dispersión y línea de regresión para relación lineal
plot1 <- ggplot(data_linear, aes(x = x, y = y_linear)) +
  geom_point(color = "#2C7FB8", size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, color = "#EF6548", linetype = "dashed", size = 1) +
  labs(title = "Relación Lineal: Dispersión y Línea de Regresión", x = "X", y = "Y (Lineal)") +
  custom_theme

# Gráfico 2: Residuos vs valores predichos para relación lineal
plot2 <- ggplot(data_linear, aes(x = Fitted, y = Residuals)) +
  geom_point(color = "#2C7FB8", size = 3, alpha = 0.8) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  labs(title = "Relación Lineal: Residuos vs Predicciones", x = "Valores Predichos", y = "Residuos Estandarizados") +
  custom_theme

# Gráfico 3: Dispersión y línea de regresión para relación cuadrática
plot3 <- ggplot(data_quadratic, aes(x = x, y = y_quadratic)) +
  geom_point(color = "#2C7FB8", size = 3, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, color = "#EF6548", linetype = "dashed", size = 1) +
  labs(title = "Relación Cuadrática: Dispersión y Línea de Regresión", x = "X", y = "Y (Cuadrática)") +
  custom_theme

# Gráfico 4: Residuos vs valores predichos para relación cuadrática
plot4 <- ggplot(data_quadratic, aes(x = Fitted, y = Residuals)) +
  geom_point(color = "#2C7FB8", size = 3, alpha = 0.8) +
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed") +
  labs(title = "Relación Cuadrática: Residuos vs Predicciones", x = "Valores Predichos", y = "Residuos Estandarizados") +
  custom_theme

# Organizar los gráficos en una cuadrícula 2x2
grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)

```


## Ausencia de multicolinealidad entre las variables dependientes {.smaller background-color="white"}

Cuando dos o más variables independientes están altamente correlacionadas:

- En estas situaciones, resulta difícil estimar cuál de las dos variables es la que explica la variable dependiente, generando errores estándar altos y baja precisión de los coeficientes calculados.  
- Para identificar esta situación, hay que revisar la matriz de correlación entre las variables y detectar correlaciones de `0,8` o más. Si este es el caso, es recomendable eliminar una de las dos variables del modelo.

---
##  Ausencia de multicolinealidad entre las variables dependientes {.smaller background-color="white"}

- También existen estadísticos que miden la presencia de multicolinealidad al correr el análisis de regresión. En particular:  
- **Factor de inflación de la varianza, VIF**: indicador de cuánto aumenta el error estándar debido a problemas de multicolinealidad.  
- Sacamos la raíz cuadrada e interpretamos el valor resultante como en cuantas veces mayor es el error estándar debido a problemas de multicolinealidad. Por ejemplo, un VIF de 4 significa que el error estándar es 2 veces mayor de lo que sería si las variables no estuvieran correlacionadas.  
- Un VIF mayor a 2.5 es considerado como indicando problemas de multicolinealidad.


## Conclusiones  {.smaller background-color="white"}  

- Los diagnósticos de regresión son esenciales para evaluar la calidad del modelo y la validez de las inferencias.
- Estadísticos como **leverage, distancia, e influencia** ayudan a identificar casos problemáticos.
- La exploración de los datos y los residuos del modelo a partir de estadísiticos y gráficos es fundamental.
- Las pruebas estadísticas adicionales, como la de **Breusch-Pagan** y **Shapiro-Wilk**, son útiles para confirmar los supuestos específicos y mejorar la confianza en nuestras conclusiones.
- Siempre es importante **documentar cualquier modificación** realizada para evitar problemas éticos o malinterpretaciones de los resultados.



